{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled2.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/suprapraba2923/HumAInSubmission/blob/master/Taxonomy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HQ5DwifbkqGQ",
        "colab_type": "code",
        "outputId": "30f56bae-c52b-499b-abbd-167696eb8ea3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import numpy as np\n",
        "from keras.TrainedModels import Sequential\n",
        "from keras.layers import Dense, Conv1D, Flatten\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.preprocessing import sequence\n",
        "from keras.utils import to_categorical\n",
        "import pandas as pd\n",
        "from collections import Counter\n",
        "from sklearn.TrainedModel_selection import train_test_split\n",
        "from nltk import word_tokenize\n",
        "from collections import defaultdict\n",
        "\n",
        "\n",
        "def CountFirstXWords(corpus, top_x, skip_top_n):\n",
        "    count = defaultdict(lambda: 0)\n",
        "    for c in corpus:\n",
        "        for w in word_tokenize(c):\n",
        "            count[w] += 1\n",
        "    count_tuples = sorted([(w, c) for w, c in count.items()], key=lambda x: x[1], reverse=True)\n",
        "    return [i[0] for i in count_tuples[skip_top_n: skip_top_n + top_x]]\n",
        "\n",
        "\n",
        "def VectorizeFirstWords(corpus, top_x):\n",
        "    topx_dict = {top_x[i]: i for i in range(len(top_x))}\n",
        "\n",
        "    return [\n",
        "               [topx_dict[w] for w in word_tokenize(s) if w in topx_dict]\n",
        "               for s in corpus\n",
        "           ], topx_dict\n",
        "\n",
        "\n",
        "def FilterFirst(corpus, n_top, skip_n_top=0):\n",
        "    top_x = CountFirstXWords(corpus, n_top, skip_n_top)\n",
        "    return VectorizeFirstWords(corpus, top_x)\n",
        "\n",
        "\n",
        "df = pd.read_csv('SmallTrain.csv')\n",
        "\n",
        "CompleteList = df['tags'].tolist()\n",
        "counter = Counter(df['tags'].tolist())\n",
        "top_10_varieties = {i[0]: idx for idx, i in enumerate(counter.most_common(10))}\n",
        "df = df[df['tags'].map(lambda x: x in top_10_varieties)]\n",
        "\n",
        "description_list = df['post'].tolist()\n",
        "mapped_list, word_list = FilterFirst(description_list, 2500, 10)\n",
        "ListOfVarieties_o = [top_10_varieties[i] for i in df['tags'].tolist()]\n",
        "ListOfVarieties = to_categorical(ListOfVarieties_o)\n",
        "\n",
        "max_review_length = 150\n",
        "\n",
        "mapped_list = sequence.pad_sequences(mapped_list, maxlen=max_review_length)\n",
        "train_x, test_x, train_y, test_y = train_test_split(mapped_list, ListOfVarieties, test_size=0.3)\n",
        "\n",
        "max_review_length = 150\n",
        "\n",
        "embedding_vector_length = 64\n",
        "TrainedModel = Sequential()\n",
        "\n",
        "TrainedModel.add(Embedding(2500, embedding_vector_length, input_length=max_review_length))\n",
        "TrainedModel.add(Conv1D(50, 5))\n",
        "TrainedModel.add(Flatten())\n",
        "TrainedModel.add(Dense(101, activation='relu'))\n",
        "TrainedModel.add(Dense(max(ListOfVarieties_o) + 1, activation='softmax'))\n",
        "TrainedModel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "TrainedModel.fit(train_x, train_y, epochs=3, batch_size=65)\n",
        "\n",
        "TrainedModel.save('TrainedModelSave.h5')\n",
        "\n",
        "# json_file = TrainedModel.to_json()\n",
        "# with open(\"TrainedModel.json\", \"w\") as file:\n",
        "#   file.write(json_file)# serialize weights to HDF5\n",
        "# TrainedModel.save_weights(h5_file)\n",
        "\n",
        "\n",
        "# input_list = {\"Hello, python, Image processing\",\"Hello, python, Image processing\"}\n",
        "# print(TrainedModel.predict(test_x))\n",
        "\n",
        "y_score = TrainedModel.predict(test_x)\n",
        "\n",
        "idxs = np.argsort(y_score[0][0])[::-1][:2]\n",
        "\n",
        "print(idxs)\n",
        "\n",
        "# y_score = [[1 if i == max(sc) else 0 for i in sc] for sc in y_score]\n",
        "# n_right = 0\n",
        "# for i in range(len(y_score)):\n",
        "#     if all(y_score[i][j] == test_y[i][j] for j in range(len(y_score[i]))):\n",
        "#         n_right += 1\n",
        "\n",
        "print(\"Accuracy: %.2f%%\" % ((n_right/float(len(test_y)) * 100)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 23.93%\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}